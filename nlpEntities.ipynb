{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8e6006-5a22-4a8b-afb8-afa850139009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authors: ['Ioannis Lambadarisa', 'Howard Schwartza', 'Michel Barbeaub']\n",
      "-----------------------------------------------------------------\n",
      "Institutions: ['ScienceDirect Neurocomputing', 'Mohammad Tayefe Ramezanloua', 'Computer Engineering', 'Carleton University']\n",
      "-----------------------------------------------------------------\n",
      "Introduction: The consensus problem in flying multi-agent systems, commonly called the flocking or swarming challenge, is fundamental to aerial robotics. It involves coordinating and controlling multiple agents to en- sure cooperative behavior, avoid collisions, and align towards a shared goal. Such coordination is crucial for applications ranging from co- ordinated surveillance to communication, logistics, and infrastructure monitoring. Formation control within these systems, especially cost-constrained communication, faces significant challenges. These include managing limited resources, overcoming communication constraints, and ensur- ing system scalability and robustness. Various studies have addressed these issues, demonstrating innovative solutions and methodologies to enhance adaptability and resilience by leveraging local sensing and communication capabilities, even on low-cost platforms . Security in multi-agent systems is also a critical concern, partic- ularly in the face of cyber threats. Adaptive mechanisms that tune communication link weights to maintain secure consensus have offered resilience in adversarial conditions . Additionally, hierarchical con- trol mechanisms, such as leader-follower dynamics, are essential for specific applications, including aerial operations. Recent advancements Corresponding author. E-mail address: MohammadTayefeRamez@cmail.carleton.ca (M.T. Ramezanlou) .include finite-time control protocols that ensure stability and rapid con- sensus in leader-follower setups and innovative path-guided control strategies that function effectively in uncertain environments . Moreover, decision-making within these systems has seen enhance- ments through entropy-based consensus methods that facilitate more efficient cooperation by employing swarm intelligence principles for local negotiation and preference updating . These methodologies promise faster convergence and scalability, which are crucial for di- verse operational settings. Overall, the ongoing research in multi-agent systems continues to address the dual challenges of robust formation control and secure, efficient consensus amidst evolving operational demands and external threats. 1.\n",
      "-----------------------------------------------------------------\n",
      "Methodology: 2.1. Consensus Flying Problem The Consensus Flying Problem deals with ensuring drones can work together in real-time to agree on their flight paths and positions. When many drones are close together, like in swarms, avoiding crashes is vital. Advanced algorithms and communication methods are needed so drones can exchange information and handle changing situations and unexpected obstacles. As shown in, a swarm of agents (follower drones) flies around a leader. The leader is controlled from a remote base station, and the swarm agents should learn to fly safely with the leader. The leader sends its position to all agents, and each agent only sees two neigh- boring agents. The swarm aims to learn how to keep a commanded distance from each other and the leader. The commanded distance is provided from the leader. Each agent uses the onboard sensors to find the distance and line of sight from neighboring agents. The follower agents are equipped with an SNN, and their learning algorithm incorporates R-STDP and FL. Each follower agent trains a local network ( ) using R-STDP and sends its model to the leader as the central server. The leader aggregates models and sends back the global model ( ). 2.2. Neuron model The LIF neuron model provides a simplified yet powerful representa- tion of neuronal dynamics. Fundamentally, using basic electrical circuit elements, the LIF model captures the behavior of a neuron s membrane potential in response to incoming currents. It incorporates membrane potential decay, realistically simulating how neurons respond to inputs with high frequencies. This enhances the model s accuracy in pre- dicting neuronal behavior under dynamic conditions, offering a more precise tool for neuroscientific research and computational simulations. Mathematically, the LIF model is characterized by a linear differential equation that describes the voltage response to a current input as follows , ( ) = ( ) + + ( ) (1) where is the membrane time-constant, is the membrane potential, is the reversal potential, is the membrane resistance, and ( )is the input current to the neuron. When the neuron s potential reaches the threshold potential ( ), it spikes, and the potential immediately returns to the resting potential ( ). For scenarios with constant input current, the Inter-Spike Interval ( ) can be analytically determined by separating variables in the governing differential equation. This allows for the derivation of the as follows , = ln( + + ) (2)Neurocomputing 617 (2025) 129005 3 M.T. Ramezanlou et al. .The central server (the leader) and the surrounding follower agents (white drones). The follower agents learn to fly in a formation to maintain the commanded distance. The local models trained individually by follower agents are sent to the leader. The leader aggregates the models and sends back the global model for another round of training on the follower agents. It is imperative to note that this solution is dependent on an input current magnitude that induces a transition of the membrane poten- tial from to . Through this analytical framework, the LIF model provides insights into the modulation of neuronal spiking dynamics based on constant input currents. The neuron does not fire when the input current is minimum and approaches infinity. This occurs when the input current is insufficient to drive the membrane potential to the threshold potential from its resting potential. By setting the to infinity, one obtains the condition: + = 0 (3) or = (4) where is the minimum input current that makes the neuron reach the potential below the threshold voltage. Conversely, the lowest pos- sible defines the maximum input current. In this paper, we consider the to be one sample time . By considering ln(1 + ) , one can derive the maximum input current that would drive the neuron to spike on every sample time as follows, = ( ) + (5) These derived equations determine the range of operation for neu- rons and establish the boundaries for the maximum weights in the SNN. Specifically, positive synaptic weights indicate the limits of excitement that a neuron can generate. In this context, the minimum input current (or synaptic weight) represents the stimulating influence a neuron can exert without triggering an action potential. In contrast, the maximum input current signifies the most powerful stimulating influence possi- ble. Conversely, considering weight values, these currents reflect the minimum and maximum inhibitory effects. In this case, the minimum and maximum input currents determine how much a neuron can inhibit other neurons from firing. 3. Proposed method 3.1. Network structure This paper assumes that each agent detects only two neighboring agents besides the leader. The information obtained from other agentsincludes the Line-of-Sight (LOS) angle and the distance. Each agent s neural network consists of three sub-layers in the input layer, as shown in. Two sub-layers correspond to the two neighboring follower agents ( 1and 2), and the third is dedicated to the leader ( ). Inputs for these sub-layers are encoded using the Gaussian Receptive Fields (GRF) that use fuzzy membership functions. The network uses the difference between current and commanded distances within the swarm ( ) and between followers and the leader ( ) to stimulate input neurons. Every input sub-layer is split into two parts. The first part deals with distances greater than the commanded distance, while the second focuses on the space between the agent and the commanded distance. Within each part, the LOS angle is encoded with fuzzy membership functions. The difference between the current and commanded distance is represented as the error. We transform this difference into an am- plitude value using the function so that it is bounded between 0 and 1. An error of zero leads to an amplitude of zero, and as the error increases towards infinity, the amplitude approaches one. Consequently, the encoding function for the input layer is expressed as follows, ( , ) =|| ( )|| exp( ( )2 2 2) (6) where and are the Gaussian membership functions center and stan- dard deviation. The is the distance from the corresponding agent, is the LOS angle, and is the vector of the membership degrees. Here, is a placeholder that can either represent or , depending on the context. The firing strengths from fuzzy encoders are then converted to the spiking input based on the neuron model as follows , =( ) ( , ) + (7) or = ( ) ( , ) + (8) The encoding process is shown in. The Fuzzy-to-Spiking (F2S) block uses (8) to calculate the inputs for the associated sub-layer. The output layer has two sub-layers, and each sub-layer has two neurons. The first sub-layer determines the , and the second one determines . The first neuron of the sub-layers is for negative values, and the second one is for positive values. Each neuron is associated with the output sign, and the magnitude of the and is encoded intoNeurocomputing 617 (2025) 129005 4 M.T. Ramezanlou et al. .SNN structure with encoding and decoding layers. Each sub-layer consists of a fuzzy encoder and the F2S Converter, with the output layer receiving inputs from synaptic weights and a random action selector. During the training phase, the output layer receives input only from the random action selector, which then shifts to synaptic weight inputs after the training. .The fuzzy encoding principle for the input sub-layer. the output sub-layers based on the minimum and maximum synaptic weights. Eq. (8) is used to encode the magnitude of the random action .Input and output of the SNN. into the output sub-layers. The only difference is that a function called is used to normalize the maximum step between 0 and 1 as follows, = (9) = (10)Neurocomputing 617 (2025) 129005 5 M.T. Ramezanlou et al. where and are selected actions, and and are maxi- mum steps (displacements) in and directions. Two random actions, one for and one for , are generated for the training process. The decoding of the spiking output is determined by the difference in the firing rates of the output neurons within each sub-layer. Let us denote ( )as the activity of the output neurons that control the movement in the and -directions: ( ) ={ 1if the neuron spikes at time , 0otherwise, The equation for decoding this activity can be expressed as: =[ = ( +( ) ( ))] (11) where +( )and ( )are the activities of the two output neurons as- sociated with the -direction. A similar process is applied for decoding in the -direction: =[ = ( +( ) ( ))] (12) where is the time window the network updates weights. One of the challenges in robotic applications is ensuring smooth transitions in actions to prevent abrupt and potentially harmful changes. Therefore, the recursive random number generation method is used to produce correlated random numbers. This method ensures that during training, the current displacements of the robot are influenced by its previous displacements, leading to smoother transitions. The recursive random number generation can be formulated as, = 1+ (1 ) (13) where is the random action at time , is a correlation coefficient, and is a random number drawn from a standard distribution (e.g., Gaus- sian) at time . This equation ensures that the random action at any given time is a weighted combination of the previous action and a new random number. 3.2. Training algorithm The R-STDP algorithm is a learning technique inspired by biological processes in the brain, which is believed to be fundamental to certain learning processes . The algorithm s core principle is that when a pre-synaptic neuron activates just before its post-synaptic counterpart, the synapse s strength connecting them should increase, and vice versa if the post-synaptic neuron fires first. Within the SNN framework, pre-synaptic neurons are the input neurons, and post-synaptic neurons function as the output neurons. The function ( )can be defined as the firing timelines of both input and output neurons as, ( ) = exp( ) for 0 (14) where stands as the exponential function s amplitude, and is the difference between the firing time of the input neuron( )and output neuron( )(see). Meanwhile, acts as the time constant, setting the decay rate for the function. Should approach infinity, the exponential function converges to 1, neutralizing time s effect on the function. The adjustment of synaptic weights follows the given equation: ( ) = ( ) ( ) (15) where ( )denotes the rate of change of the synaptic weight that connects neurons and . This weight determines the input that the post-synaptic neuron receives upon the spiking of its pre-synaptic neu- ron, which is quantified as ( ). The term ( )represents the reward that is received at time .The reward functions used in this paper are as follows, ( ) = [ ( 1) ( )] ( ( ) ) (16) ( ) = [ ( 1) ( )] ( ( ) ) (17) where, , , and denote the reward, distance, and com- manded distance between two and follower agents, respectively. Similarly, , , and represent the reward, distance, and the commanded distance between the follower agent and the leader ( ), respectively. The terms and are the reward coefficients and the ( ( ) )and ( ( ) )functions determine the reward s sign according to the agents relative distance and the com- manded distance. The expressions ( 1) ( )and ( 1) ( ) specify the magnitude of the instantaneous reward. If an agent finds itself farther away from the commanded distance than a neighboring agent or the leader, it will be rewarded positively for decreasing its distance. Conversely, moving closer results in a negative reward if the agent is within the commanded distance from a neighboring agent or the leader. This system is designed to encourage the maintenance of a commanded distance: being too far away from the commanded distance invites a penalty. At the same time, positive reinforcement is given for closing the gap between the current distance and the commanded distance. One of the challenges in R-STDP is the unbounded growth or decay of synaptic weights, which can impede effective learning in neural networks. The following section introduces a novel weight-stabilization method to address this challenge and enhance the algorithm s applica- bility. 3.2.1. Weight stabilization using reward-modulated competitive synaptic equilibrium (R-CSE) Controlling the excessive increase of synaptic weights in SNNs is im- portant to maintain network resilience and function. If not controlled, this growth can lead to saturation, affecting the network s ability to learn and adapt. When the network receives fuzzy sets of firing strengths as input, the synaptic weights grow in a pattern influenced by the Gaussian function s shape used for fuzzy encoding. Imposing a limit on synaptic weights disrupts this growth pattern over time, and eventually, all the synaptic weights reach the maximum. Weight normalization, while preventing excessive growth in one part of the net- work, can inhibit overall growth; when a synaptic connection reaches its maximum, its activation subsequently diminishes other weights. Traditional methods like L1 regularization and weight decay employ a constant decay rate, which can slow the network s responsiveness to changes in rewards. Alternatively, a more advanced approach, the Bienenstock, Cooper, and Munro (BCM) method, dynamically adjusts both a threshold and a decay rate in response to input variations. However, this method does not provide a control mechanism for the fuzzy inputs. In this chapter, we introduce a method called R-CSE to manage the unbounded growth of synaptic weights while maintaining the gradual change in the synaptic weights formed due to differences in firing strength from fuzzy membership functions. Our method also dynamically adjusts the network when the reward changes by adjusting the maximum synaptic weight based on the reward. The enhanced version of the R-STDP method considering the control mechanism from R-CSE algorithm is expressed as follows, ( ) = ( ) ( ) sgn( ) (18) where is the Hadamard product, is the learning rate matrix, and is the decay rate matrix. The primary distinction between the R-CSE method and the approach detailed in Section 2.4 lies in the decay rate, which allows the learning system to remain adaptable after the learning phase, and in the learning rate, which is represented as a matrix rather than a scalar value affecting all synaptic weights uniformly. These modifications enhance the learning algorithm s flexibility in respondingNeurocomputing 617 (2025) 129005 6 M.T. Ramezanlou et al. to reward changes and provide greater control over synaptic weight adjustments. Let us define as the set of input and output neurons that fired at time in one of the network sections. If we consider ( )as the maximum weight among the firing neurons in set , then we can characterize the learning rate using a Sigmoid function. The learning rate value ( ( )) gradually transitions from 1 to 0 as the learning process advances, as explained below: ( ) =1 1 + exp[ 1 (| ( )| )],( = ) (19) where is the maximum reward in the network section (e.g., ( )), = ( , ), and is a small positive number that controls the curvature of the function around ( ) = . This model determines the learning rate by the highest synaptic weight among the active input and output neurons. This mechanism is similar to the winner-takes-all approach. When a synaptic connection reaches its weight limit, it prevents further changes in the adjacent synaptic weights. The network contains a variety of reward functions, each with its own maximum and minimum values. The highest reward value in a specific area of the network sets the limit for the synaptic weight in that area. The synaptic weight limit is linked to the ratio of the local maxi- mum reward ( ) to the global maximum reward ( ). As a result, the network section with the highest local maximum reward ( = ) attains the maximum allowable synaptic weights because = 1, while sections with lower local maximum rewards reach only a proportional fraction of the maximum weight. The adjustment of the learning rate transforms into a competitive algorithm that modifies the growth rate of individual synaptic weights by considering network parameters, like reward and maximum synaptic weight. A significant challenge in learning algorithms is their capacity to adapt to changes in rewards. Commonly, once the learning rate reduces to zero, weight adjustments stop. To address this, a variable decay rate is introduced to prevent weights in each network section from indefinitely remaining at their peak values. In our method, the decay rate is represented as a matrix, and it is calculated using the SoftPlus function, enabling it to adjust according to the current stage of learning. This method ensures that weight modifications continue to respond effectively to changes in the learning environment. This chapter defines the decay rate as a function of the maximum synaptic weight among neurons in the set . This approach is designed to address a critical aspect: when the maximum synaptic weight in reaches its peak (| ( )|= ), it is essential that the learning rate remains above zero. This condition is necessary to allow weight change and prevent the learning rate from stagnating at zero ((18)). Simultane- ously, the learning rate must not exceed the maximum acceptable rate of weight change, which is . When the reward coefficients change after training, it can cause| ( )|to exceed for set . With these considerations, we propose that the decay rate should be set to when| ( )|= and increase to when| ( )|= 2 , where is a coefficient that controls the rate of decay when| ( )|> . By applying the mentioned condition and solving for the SoftPlus function, the decay rate function can be obtained as follows, =( ) log(1 + exp[ (| ( )| )])(20) where = ln (2 2 1) log (2)is a scaling parameter that can adjust the output scale of the function, and =ln (2 2 1) controls the curvature of the function. A higher makes the SoftPlus function approach a step function, making it closer to the binary behavior. Conversely, asmaller makes the function smoother and more gradual. Eq. (20) can be represented as, =( log (2)) log( 1 + exp[( ln (2 2 1) ) (| ( )| )]) (21) The choice of setting the decay rate to when| ( )|= 2 is based on the feature of reward coefficients. Specifically, when the reward coefficients in (16) and ( 17) increase, leading to new condition where or change, the| ( )|is allowed to increase. Conversely, a decrease in the reward coefficient, resulting in | ( )|> , necessitates a higher decay rate to reduce the| ( )| back to . When| ( )|< , the reward adjusts the synaptic weights, and there is no weight decay to disturb the learning process. When | ( )|> , the decay rate changes the synaptic weights and brings the maximum weight to the reward zone, where| ( )|< and the networks responds to reward change. Lemma 3.1. The R-CSE method is asymptotically stable in its equilibrium point ( ) = . Proof. We consider the dynamical system given by the equation of synaptic weights for ( ) 0that receives a positive reward ( ( )> 0) as, ( ) =1 1 + exp[ 1 ( ( ) )] ( ) ( ) ( log (2)) log( 1 + exp[( ln (2 2 1) ) ( ( ) )]) , (22) To assess the stability of this system around the equilibrium point, we introduce a Lyapunov function candidate ( ), where = ( ) . A common choice for such analyses is a quadratic function of the deviation from the equilibrium: ( ) =1 2 2. (23) This positive definite function has a minimum at the equilibrium point, satisfying the essential criteria for a Lyapunov function. The derivative of ( )with respect to time, ( ), is then calculated to determine the rate of change of the Lyapunov function along the trajectories of the system: ( ) = (24) Substituting ( )from ( 22) into the above expression, we have, ( ) = 1 1 + exp[ 1 ( )] ( ) ( ) ( log (2)) log(1 + exp [( ln (2 2 1) ) ( )]) (25) A negative ( )indicates that the system s energy decreases over time, concluding that the equilibrium point is asymptotically stable. Conversely, a positive ( )in any region would suggest the presenceNeurocomputing 617 (2025) 129005 7 M.T. Ramezanlou et al. .Synaptic weight change for = 5, = 15.5, and = 1. of instability or regions of attraction that do not encompass the entire state space. In analyzing the system s stability, we focus on the behavior of the derivative of the Lyapunov function, ( ), across different regions of . We decompose the dynamics of into its constituent components to systematically analyze the stability conditions. We assess the relative magnitudes of the two main components influencing ( ): 1. The first term, represented as1 1+exp[1 ] ( ) ( ), denotes the effect of learning rate and is inherently positive when the synaptic connection receives positive reward consistently. 2. The second term,( log (2)) log( 1 + exp[( ln (2 2 1) ) ]) , cap- tures the dynamic decay rate of synaptic weights, which is governed by the SoftPlus function. Analysis for <0: In this region, we observe that the termexp[( ln (2 2 1) ) ] goes to zero and makes the term inside thelogfunction go to 1. This implies that the contribution of this term to ( )is negligible in this region. Moreover, the first term remains positive throughout, and given that it is multiplied by (which is negative in this region), the overall contribution to ( )is negative. Consequently, ( )is negative for <0, indicating that any perturbations from the equilibrium in this region will decrease over time, thereby contributing to the system s stability. Analysis for >0: For this region, the magnitude of the second term significantly exceeds that of the first term. This predominance is critical as it is associated with a negative sign in the ( )equation. Therefore, the negative contribution of this component ensures that ( )remains negative throughout this region. It indicates that any deviation from the equilibrium state results in the system s energy decreasing over time, leading to the conclusion that the equilibrium point ( ) = is asymptotically stable for >0. demonstrates the performance of the R-CSE when it regulates the synaptic weights to prevent unbounded growth. The red dotted line represents the value of , which is derived from the maximum reward value of the corresponding network section. As shown in, the synaptic weight oscillates around , and when it drops below , the learning rate is set to 1 by (19). This allows any changes in the reward function to be applied to the synapse. .Reward-based learning rate and decay rate functions. In the blue region (active learning rate), the reward adjusts the weights, and in the red region (active decay rate), the RCSE method controls synaptic growth. According to, when , synaptic weights in set increase. If the reward changes, also changes. Depending on the current value of , the R-CSE either increases or decreases the synaptic weights within set . shows how the maximum synaptic weight of the active synapses in set stops the adjacent synaptic connections growth by setting the learning rate of the set to 0. 3.2.2. Federated learning for consensus flying In FL, a key challenge is centralizing various models on one server. This process must effectively combine these models to create a unified global model without compromising the specific adjustments made to each model. A critical strategy involves choosing models that contain substantial information. Another significant aspect is determining the frequency of model aggregation. Shorter intervals between aggrega- tions can enhance learning efficiency but may strain network resources, particularly as the number of participating agents and devices grows. Conversely, longer intervals might slow down the learning process due to delayed updates of the global model. This section proposes an aggregation method for SNN. Our focus is on reducing network usage and energy consumption. This approach allows clients to upload their local model updates at different times rather than synchronously. Such a method is particularly beneficial in reducing the negative impacts of device heterogeneity, which can include varying computational capacities and network con- nectivity among devices . In traditional FL setups, delays caused by poor network signals or unexpected client crashes can significantly prolong the time the server takes to receive updates from all clients. By adopting asynchronous aggregation, the server processes and ag- gregates models as they are received without synchronizing with all clients. This strategy accelerates the training process, making FL more efficient and adaptable to diverse client conditions. Our proposed FL model aggregation algorithm aims to establish an efficient and event-triggered system for global and local model pub- lishing. This system relies on the similarity between consecutive global and local models and publishes updates only when significant changes are detected, thus avoiding redundant updates and improving overall efficiency. Unlike the uniform model updates in FedAvg [47,48], our approach allows individual agents to evaluate and send their local models based on a similarity threshold with the global model, thereby enabling a potentially more effective update process. Our aggregationNeurocomputing 617 (2025) 129005 8 M.T. Ramezanlou et al. .RCSE working principle in inhibiting the adjacent synaptic connections. The heatmap shows the synaptic weight matrix. Neurons have different firing strengths due to the difference in fuzzy membership values, which affects the increase or decrease rate and shapes the patterns in the synaptic weight matrix. strategy emphasizes similarity metrics for model updates, which is not commonly emphasized in methods like FedNova , adding a layer of context sensitivity to our approach. In our approach, considering the difference in agents neural net- work parameters and maximum and minimum synaptic weights, the weights are normalized to align them on a uniform scale ranging from 1 to 1. This normalization process makes the neural model values comparable across the network. Based on the maximum and minimum synaptic weights outlined in (5), and taking into account the highest excitation ( ) and inhibition ( ), the normalization of synaptic weights is performed as follows: ( ) =1 [ ( )] (26) where ( )represents the matrix of synaptic weights, ( )denotes the normalized synaptic weight matrix for agent {1,2,3, ..}, and is the maximum synaptic weight for agent . The global model on the server (Leader) is then computed using a weighted average, ( ) = =1 . ( ) =1 (27) where ( )is the global normalized model on the central server, is the number of agents, and is the aggregation weight for each SNN model, defined as, =1 ( ) exp( ) ( ) (28) where the term . is the Frobenius norm, and and are the dimensions of the matrix ( ), used for normalizing the Frobenius norm. indicates the time at which agent last transmitted its local model to the central server, and is a time constant that reduces the weight to zero if there is no recent update from the agent. Both agents and the central server employ an event-triggered mech- anism for transmitting local and global models. Throughout the trainingphase, each agent calculates the Euclidean distance between the most recent global model from the central server and its current synaptic weights matrix, as follows, ( ( ), ( )) =1 2 =1 =1( )2 (29) where is the Euclidean distance on the agent side, is the time when the central server published the global model, and and are elements of the latest global model and the current local model, respectively. If this distance exceeds a certain threshold, set between 0 and 1, the agent transmits its model to the central server. If the on the agent reaches the threshold and it does not receive any update from the server, the agent sends its model to the server, and then it calculates the between current synaptic weights ( )and the model it recently sent to the server ( )until it receives a new model update from the central server. The central server follows a similar procedure as the agents, evaluat- ing the distance between the current and recently published model at time , ( ( ), ( )) =1 2 =1 =1( )2( ) (30) Incorporating the proposed FL method with the R-CSE algorithm, the modified R-STDP equation can be represented as follows, ( ) = (1 ( ))[ ( ) ( ) sgn( ( ))] + ( ) ( ( ) ( )) (31) where is the Dirac delta function. Algorithm 1shows the step-by-step implementation process of the proposed method.Neurocomputing 617 (2025) 129005 9 M.T. Ramezanlou et al. Algorithm 1High-Level Algorithm for the Proposed FL Algorithm Require: Initialization of Central Server and Agents Ensure: Updated Global Model on the Central Server and Local Models on Agents 1:Initialize the agents and Central Server with default parameters for model publication threshold, Euclidean distance, and model publish status 2:Initialize the Global Model on the Central Server 3:if is greater than 0then 4: Normalize synaptic weights of local models using (26) 5: Aggregate models from all agents at the Central Server using (27) 6: Calculate the between the current and previous global models on the Central Server using (30) 7: if >the Central Server s threshold then 8: Publish the global model 9: set = 10: end if 11: foreach Agent in the network do 12: ifCentral Server publishes a new global model then 13: Update the local model of the Agent with the global model using (31) 14: else 15: Agents evaluate their local models against the latest global model ( ) using (29) 16: if >the Agent s threshold then 17: Send the model to the Central Server 18: set = 19: end if 20: end if 21: end for 22:end if The proposed algorithm allows agents to communicate less of- ten and save energy. It only sends essential updates to the Central Server, which helps when many agents have different SNN models and communication interfaces. This method reduces unnecessary data transmission, making the whole system more efficient. 4. Results and discussion In this section, we conducted a numerical simulation to validate the performance of the proposed method. The simulation involves a group of five agents flying around a leader who is moving in a circular path. Initially, a scenario without implementing FL was conducted to evaluate the performance of the SNN in achieving coordinated flight. During this phase, the effect of the change in reward was simulated to examine the R-CSE method. In the second part of the simulation, the proposed FL aggregation algorithm is used, where the leader agent acts as a central server. Finally, the algorithm was tested both before and after changing the rewards. 4.1. Simulation without FL In this simulation, we modeled five agents, each equipped with its own SNN model, capable of reaching a maximum speed of 1 m/s. The architecture of each agent s neural network included 72 input neurons. Since each agent was designed to detect three distinct objects within its environment, the input layer was organized into sub-layers, with 24 neurons dedicated to each object. The network s output layer comprised 4 neurons, divided equally to represent and movements. The SNN model in the simulation is a fully connected network, and the parameters of the LIF neuron are also represented in. The R-STDP mechanism updated synaptic weights at 10 ms inter- vals. During these intervals, the learning algorithm adjusted the agent s states based on received data from other agents and the leader while Parameter values for LIF neuron model . Parameter Value Description 40 M Membrane Resistance 30 ms Membrane time constant 70 mV Resting potential 70 mV Reset potential 0 70 mV Initial membrane potential 50 mV Threshold membrane potential Simulation parameters. Parameter Value Description 10 ms Weight and state update sample time 2 ms Time constant for R-STDP 1 Amplitude in R-STDP function 5 Decay rate coefficient and 0.01 m Max step per 0.5 Gaussian function s std. deviation 1 ms Minimum inter-spike interval 0.5 Lower bound of synaptic weight 15.5 Upper bound of synaptic weight 0.95 Correlation Coefficient .Measured distances used for evaluating swarm flight performance and collision detection. simultaneously generating random outputs as part of an exploration strategy. shows the simulation parameters. The simulation was done in a 10 m by 10 m area, and the leader followed a circular path centered at (5,5) with a 2.5 m radius and a 0.1 m/s speed. To monitor swarm performance, the minimum and maximum dis- tances of each agent from other agents and the minimum and maximum distances of the swarm from the leader were measured. shows the definition of the distances. The simulation included two phases. During the initial phase, the objective was for the agents to learn to maintain the commanded distance from each other and the leader. This phase took 600 s for training, and the reward coefficient among followers( )was set at 0.02, while the coefficient between followers and the leader( )was set at 0.07. These parameters were derived from a series of numerical simulations. A higher value of signifies an increased emphasis on the leader in the learning process, which means that the distance to the leader is more important than the commanded distance between agents. shows the simulation results for the R-CSE method. According to the results, the agents rapidly aligned around the leader within 6.89 s, and the maximum distance was reduced from 7.632 meters to the target distance of 2 m. The swarm completed the formation around the leader in approximately 8.94 s, avoiding collisions (see). As mentioned in Section 3, the input encoding uses the error be- tween current and commanded distance. Therefore, one of the ad- vantages of the encoding and learning method in this paper is that the learned policies are independent of the commanded distance. The commanded distance can be changed after training since the SNN uses the distance error.Neurocomputing 617 (2025) 129005 10 M.T. Ramezanlou et al. .Agents trajectory during the test phase. .Variation of distances within the swarm during the test phase. .Adaptive response to commanded distance adjustments: dynamic reconfigura- tion during the test phase. .Trajectory adaptations of following agents in response to reward change for the leader during the test phase. Another key feature of our SNN implementation is that when the agent successfully aligns with the commanded distances, resulting in zero error, the input neurons cease to spike. This characteristic lever- ages the sparsity of spikes in SNNs, significantly reducing energy con- sumption, as the spike is the primary energy consumer in these net- works . This absence of spiking under zero-error conditions demon- strates the network s precision and operational efficiency, as it min- imizes unnecessary computational activity and power usage. While quantized ANNs are beneficial in reducing model size and computa- tional demands, they often face challenges in maintaining accuracy due to reduced precision, which can be critical in complex decision-making contexts. shows the agents response to changes in commanded dis- tance after training. According to this figure, when the commanded distance is changed at 30 s, the swarm immediately responds to this change in 2.98 s without disrupting the formation or any collision. After 600 s, the leader is changed into an obstacle, and its reward coefficient is changed to 0.0175. The reward sign function,t anhin (17), is also changed to 1, so the reward function for the leader is changed as follows, ( ) = [ ( 1) ( )](32) When the leader is transformed into an obstacle, the encoding equation for the input layer must be changed. This is because the obstacle has no commanded distance, and the agents must maintain a commanded distance only from each other. Therefore, the commanded distance from the obstacle encoder in the input layer must be removed. Therefore, ( 6) can then be rewritten as follows: ( ) = exp( ( )2 2 2) (33) The simulation proceeded for an additional 1200 s, during which the synaptic weights were adjusted in accordance with the new reward function given by (32). The results of the reward change are shown in Figs. 12and13, which indicate that the agents quickly reduced their initial distance to the commanded distance of 2 m. Simultaneously, the minimum distance from the obstacle, the leader, increased over time, indicating that the agents adapted their behavior to maintain a greater distance from the obstacle. shows the trajectory of each agent after the reward change. In order to better understand the effect of reward change on the SNN, the synaptic weights matrix before and after reward changeNeurocomputing 617 (2025) 129005 11 M.T. Ramezanlou et al. .Variations in distances after reward changes and Leader becomes Obstacle - test phase. has been illustrated inFigs. 14and15, the vertical axis shows the output neurons. The first output neuron is for negative displacement in the -direction, while the second output neuron is dedicated to positive displacement in the -direction. Similarly, the third output neuron corresponds to negative displacement in the -direction and the fourth output neuron to positive displacement in the -direction. The horizontal axis shows the input neurons. The neuron IDs from 1 to 24 are for the first sub-layer dedicated to the neighboring follower. The network has two sub-layers for the neighboring follower agents, but only one is shown since they are similar in the case of synaptic weight values. The neuron numbers from 25 to 48 are for the sub- layer dedicated to the leader. The R-CSE method aims to maintain the synaptic weight matrix gradient while adapting to changes in the reward signal. Considering the numerical values presented inalong with the reward coefficients = 0.02and = 0.07, and = 1 m sand = 0.1 m s, the maximum rewards at each weight update interval ( ) for and are calculated using (16) and (17) as4 10 4 and7.7 10 4, respectively. Consequently, = max( , ) = 7.7 10 4. The for the follower section in the network is[ 4 10 4 7 10 4] 15.5 = 8.0519, and for the leader section, it is[ 7 10 4 7 10 4] 15.5 = 15.5. The and for the follower section within the network are ln (2 2 1) log (2)= 0.0011 and =ln (2 2 1) = 2.152, respectively. For the leader section, these values are ln (2 2 1) log (2)= 5.719 10 4and =ln (2 2 1) = 1.118, respectively. The visual patterns observed in the synaptic weights matrix in Figs. 14and15, specifically, the gradual increases and decreases in values across weights, directly result from applying Gaussian member- ship functions for encoding. As illustrated in the heatmap visualization, regions of higher values denote areas closer to the function s center, where the degree of membership peaks. Conversely, areas of lower val- ues reflect points moving away from the center, where the membership degree decreases according to the Gaussian distribution s tails. Since the reward coefficients for followers and leaders are differ- ent, their maximum allowed synaptic weights are also different. The proposed method for controlling the unbounded growth of synaptic weights has successfully stabilized the network. shows the synaptic weights after the reward change. In this case, since the reward coefficients are changed, the and values are changed for the represented sub-layers, and the proposed method has .Synaptic Weights before Reward change. .Synaptic Weights after Reward change. .Synaptic weights increase after reward change. helped the R-STDP algorithm to adjust the weights based on the new situation in the environment. Figs. 16and17show the changes in the synaptic weights before and after the reward change for each section of the neural network.Neurocomputing 617 (2025) 129005 12 M.T. Ramezanlou et al. .Synaptic weights decrease after reward change. According to the figures, the maximum synaptic weight converges to the maximum threshold defined based on each section s reward value. 4.2. Simulation with FL and R-CSE In this section, the proposed aggregation algorithm is tested. In this case, the agents only send their models when the Euclidean distance between the current and previously published model or the latest global model reaches a threshold. In the first phase, the simulation was done in 600 s, and the agents learned to follow the leader. The threshold for publishing the agents and server models was 0.0005 and 0.00051, respectively. The reason for choosing the server s threshold higher than the agents is that as soon as the first agent sends its model to the server, the Euclidean distance between the current and previously published model by the server reaches 0.0005, and the server distributes the model immediately. Therefore, the serve s threshold is set higher than the agents threshold, so it waits for the other agents to send their models. shows the distances between agents and the leader before the reward change. According to the figure, the agents converge to the solution faster than the non-federated learning scenario without any error. shows the simulation results for the reward change scenario. According to the figure, the proposed event-triggered FL method has improved the learning performance so that the swarm converges to the solution in 6 s. To verify the method s effectiveness, the number of agents is in- creased to 7.Figs. 20and21show the performance of the proposed method in formation flying. According to, the agents can fly around the leader without colliding with each other. Because of the complexity of the environment in this case, it takes more time (around 22 s) for the agents to stabilize the formation. As shown in, the norm of the synaptic weights can represent the changes in the synaptic weights due to the change in the envi- ronment during the training process, which can be used to adjust the learning process in SNNs. In the proposed event-triggered FL method, agents communicate with the Central Server (leader) during training. According to, the aggregation step time is small at the beginning of the training and increases as the SNN models converge to the final solution. The rate of change of the norm of the synaptic weights determines the communication sample time of the aggregation process, which results in small aggregation intervals when the change rate is high and larger intervals when it reduces. Therefore, the aggregation frequency is very high initially, and it reduces after the change in synaptic weights goes to zero because of .Distance measurements during the test phase before reward change in the proposed event-triggered FL method. .Distance measurements during the test phase after reward change in the proposed event-triggered FL method. .Formation flying of 7 agents in the proposed event-triggered FL method.Neurocomputing 617 (2025) 129005 13 M.T. Ramezanlou et al. .Distances between 7 agents and the leader in the proposed event-triggered FL method. .Frobenius norm of the Agent 1 during the learning phase. The reward changes for the Leader after 600 s. .Communication times for agents and the Central Server (Leader). Red and blue dots show the times that agents and the Central Server have sent their model, respectively. Comparative performance analysis of the proposed aggregation Algorithm+R-CSE and R-CSE. FL+R-CSE R-CSE Learning Time - Training Phase (s) 261.92 554.71 Max Distance Convergence Error - Test Phase (%) 0.71 1.95 Convergence Time for Distance - Test Phase (s) 5.81 8.94 the learning rate in (19). Also, after the reward changes and the leader becomes an obstacle after 600 s, the Euclidean distance between the converged and current models increases and reaches the threshold. As soon as the first agent sends its model to the server, the aggregation process starts again, and the agents adjust the associated synaptic weight. compares the results and focuses on three critical metrics: learning time, maximum error after convergence, and convergence time. The proposed FL algorithm demonstrates significant improve- ments in terms of efficiency and accuracy, as evidenced by its consid- erably shorter learning and convergence times and a notable reduction in error after convergence. Our approach advances beyond traditional FL frameworks by intro- ducing an innovative event-triggered aggregation mechanism tailored for the real-time adaptive capabilities inherent in SNNs. This method contrasts with those seen in literature, where aggregation weights are primarily calculated based on rewards . Instead, our model considers both the time of arrival and the substantive content of each model, quantified through the Euclidean norm of the synaptic weights, to determine the aggregation weight, providing a nuanced approach to model integration. Additionally, while other studies address challenges related to abrupt model changes due to global model updates using fixed aggregation intervals, our model inherently avoids these disruptions through its event-triggered mechanism [53,54]. This is evident from the smooth transitions observed in the plot of the Euclidean norm of each agent, illustrating a more stable model evolution without the sudden changes typically associated with periodic global updates. In our framework, agents autonomously decide the optimal time to send their updates based on real-time changes, effectively resolving issues related to model selection and synchronization prevalent in systems with predetermined aggregation schedules. Moreover, most of the papers employ methods like Deep Q-Network (DQN) requiring a data buffer for training ; our use of an SNN facilitates an online learning paradigm. This online method allows our model to continually learn and adapt without storing data, enhancing efficiency and suitability for real-time applications. Through these distinctions, our approach improves communication efficiency and learning adaptability and provides a robust solution for dynamic and distributed learning environments, setting a new standard for deploying federated learning in multi-agent systems. 5. Conclusion In this paper, we presented a comprehensive approach addressing the challenges of uncontrolled growth in synaptic weights and the lim- ited responsiveness of R-STDP to real-time changes within SNNs. Our proposed solution integrates the R-CSE method with a dynamic aggre- gation interval in FL and significantly reduces learning time while im- proving performance. The R-CSE method introduces a novel mechanism to manage the unbounded growth of synaptic weights by dynamically adjusting the decay rate through the SoftPlus function. This adjustment is sensitive to the learning stages and rewards changes, ensuring synap- tic weight adjustments remain responsive over time. By addressing the challenge of synaptic weight saturation, the R-CSE method facilitates a balanced approach to weight adjustment, preventing network satura- tion and promoting continuous learning adaptability. We introduce aNeurocomputing 617 (2025) 129005 14 M.T. Ramezanlou et al. novel approach that uses FL in SNN and employs the Frobenius norm to adjust weighted aggregation in FL. Additionally, we include weight decay proportional to the time elapsed since an agent s last model publication. This improves the efficiency and responsiveness of the learning process. Our model s dynamic nature of the model aggregation time adjusts based on the Euclidean norm. This metric measures the distance between the weight matrices of the agents and the server, determining reduced intervals for model publication. Our results show that the proposed aggregation method significantly accelerates agents learning while increasing the accuracy of the swarm in following the commanded distance. Moreover, the dynamic aggregation interval effectively reduces communication overhead between the agents and the central server, particularly after model convergence. This reduction is critical when communication bandwidth is limited or costly. This approach is particularly advantageous in 5G networks, where efficient bandwidth use can enhance the overall throughput and reduce latency in real-time applications. Moreover, the adaptive use of communication resources aligns with the scalable and flexible infrastructure of 5G, optimizing network performance even during peak demand periods.\n",
      "-----------------------------------------------------------------\n",
      "Keywords: ['Spiking neural network STDP Federated learning Consensus flying Leader follower flocking']\n",
      "-----------------------------------------------------------------\n",
      "Abstract: This paper introduces a novel approach to enhance the stability and efficiency of R-STDP in the context of federated learning. The primary objective is to stabilize the unbounded growth of R-STDP and make it more responsive to real-time changes. The methodology involves integrating R-STDP with Spiking Neural Networks and employing the norm of the neural network model for adjusting weighted aggregation in federated learning systems. The proposed method incorporates a mechanism where weights decay over time, depending on the duration since the agent last published its model. Additionally, the sampling time is dynamically adjusted based on the Euclidean norm, which measures the distance between the weight matrices of the agents and the server. The results demonstrate that the proposed event-triggered federated learning method significantly enhances learning speed and performance. At the same time, the dynamic aggregation interval efficiently reduces communication between the agents and the central server, especially after model convergence. This research presents a significant advancement in federated learning and offers a more stable, responsive, and efficient learning process.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)  # Remove references like [1], [2]\n",
    "    text = re.sub(r'(Figure|Table|Fig\\.) \\d+', '', text, flags=re.IGNORECASE)  # Remove figure/table mentions\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_text_before_article_info(text):\n",
    "    match = re.search(r'(.*?)A\\s*R\\s*T\\s*I\\s*C\\s*L\\s*E\\s*I\\s*N\\s*F\\s*O', text, re.DOTALL | re.IGNORECASE)\n",
    "    return match.group(1).strip() if match else \"\"\n",
    "\n",
    "def segment_text(text):\n",
    "    sections = {}\n",
    "\n",
    "    # Keywords\n",
    "    keywords_match = re.search(r'Keywords:\\s*(.*?)A\\s*B\\s*S\\s*T\\s*R\\s*A\\s*C\\s*T', text, re.DOTALL | re.IGNORECASE)\n",
    "    sections[\"keywords\"] = keywords_match.group(1).strip() if keywords_match else \"\"\n",
    "\n",
    "    # Abstract\n",
    "    abstract_match = re.search(r'A\\s*B\\s*S\\s*T\\s*R\\s*A\\s*C\\s*T\\s*(.*?)1\\.\\s*Introduction', text, re.DOTALL | re.IGNORECASE)\n",
    "    sections[\"abstract\"] = abstract_match.group(1).strip() if abstract_match else \"\"\n",
    "\n",
    "    # Introduction\n",
    "    intro_match = re.search(r'1\\.\\s*Introduction\\s*(.*?)1\\.\\s', text, re.DOTALL | re.IGNORECASE)\n",
    "    sections[\"introduction\"] = intro_match.group(1).strip() if intro_match else \"\"\n",
    "\n",
    "    # Methodology \n",
    "    methodology_match = re.search(\n",
    "        r'2\\.\\s*Preliminaries\\s*(.*?)\\s*CRediT',\n",
    "        text,\n",
    "        re.DOTALL | re.IGNORECASE,\n",
    "    )\n",
    "    sections[\"methodology\"] = methodology_match.group(1).strip() if methodology_match else \"\"\n",
    "\n",
    "    return sections\n",
    "\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    authors, institutions = set(), set()\n",
    "\n",
    "    # Extract authors and institutions\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            authors.add(ent.text.strip())\n",
    "        elif ent.label_ in [\"ORG\", \"FAC\"]:\n",
    "            institutions.add(ent.text.strip())\n",
    "\n",
    "    return {\n",
    "        \"authors\": list(authors),\n",
    "        \"institutions\": list(institutions),\n",
    "    }\n",
    "\n",
    "# Main Pipeline\n",
    "def extract_information(pdf_path):\n",
    "    # extract text from pdf\n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "    # Clean text\n",
    "    cleaned_text = clean_text(raw_text)\n",
    "    # Extract text before \"article info\"\n",
    "    text_before = extract_text_before_article_info(cleaned_text)\n",
    "    # Segment text into sections\n",
    "    sections = segment_text(cleaned_text)\n",
    "\n",
    "    # Extract authors and institutions\n",
    "    entities = extract_entities(text_before)\n",
    "\n",
    "    return {\n",
    "        \"authors\": entities[\"authors\"],\n",
    "        \"institutions\": entities[\"institutions\"],\n",
    "        \"introduction\": sections.get(\"introduction\", \"\"),\n",
    "        \"methodology\": sections.get(\"methodology\", \"\"),\n",
    "        \"keywords\": sections.get(\"keywords\", \"\").split(\", \"),  # Split keywords by commas\n",
    "        \"abstract\": sections.get(\"abstract\", \"\"),\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"Guesbaya.pdf\"  # Replace with your file path\n",
    "    extracted_info = extract_information(pdf_path)\n",
    "    print(\"Authors:\", extracted_info[\"authors\"])\n",
    "    print(\"-----------------------------------------------------------------\")\n",
    "    print(\"Institutions:\", extracted_info[\"institutions\"])\n",
    "    print(\"-----------------------------------------------------------------\")\n",
    "    print(\"Introduction:\", extracted_info[\"introduction\"])\n",
    "    print(\"-----------------------------------------------------------------\")\n",
    "    print(\"Methodology:\", extracted_info[\"methodology\"])\n",
    "    print(\"-----------------------------------------------------------------\")\n",
    "    print(\"Keywords:\", extracted_info[\"keywords\"])\n",
    "    print(\"-----------------------------------------------------------------\")\n",
    "    print(\"Abstract:\", extracted_info[\"abstract\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1655ff41-b443-4b40-8255-7035498715fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
